{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skript zum Cleanen, Splitten und Parsen der Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from utils import preprocessing\n",
    "import re\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "import spacy\n",
    "import datetime as dt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einlesen der Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Datensatz wird in ein Pandas Dataframe eingelesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df = pd.read_json('../2019-02-19_oldp_cases.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Pandas Dataframe wird in eine Python Liste umgeformt und anschließend gelöscht, um Arbeitsspeicher freizugeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = []\n",
    "for i in range(len(cases_df)) : \n",
    "    cases.append(cases_df.loc[i].to_dict())\n",
    "    \n",
    "del cases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanen der eingelesenen Cases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methode: clean\n",
    "\n",
    "IN: Der zu cleanende Content\n",
    "\n",
    "OUT: Der gecleante Content\n",
    "\n",
    "In dieser Funktion werden unter anderem die HTML-Tags sowie Whitespaces und XML-Entity-Schreibweisen entfernt. Zudem werden zwischen Aufzählungen und dem nachfolgenden Text jeweils ein Punkt sowie ein Leerzeichen eingefügt, um die Aufzählung vom Text zu separieren. Ist bereits ein Punkt nach der Aufzählung vorhanden, so wird nur ein Leerzeichen ergänzt. Selbes gilt für Aufzählungen gefolgt von Anführungszeichen.\n",
    "Potentielle Tippfehler, wie etwa \",.\" werden durch \".\" ersetzt, \":.\" durch \":\".\n",
    "\n",
    "\"\\n\" werden bewusst weiterhin im Text behalten und anschließend in der Methode \"get_case_dict\" benutzt um zusätzliche Satztrennungen durchzuführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(content):\n",
    "    soup = BeautifulSoup(content)\n",
    "    content = soup.get_text()\n",
    "    content = preprocessing.remove_whitespace(content)\n",
    "    \n",
    "    content = re.sub(r'(\\d)([a-zA-ZäöüÄÖÜß])', \"\\g<1>. \\g<2>\", content)\n",
    "    content = re.sub(r'(\\d)(\\.)([a-zA-ZäöüÄÖÜß])', \"\\g<1>\\g<2> \\g<3>\", content)\n",
    "    content = re.sub(r'(\\d)(\\„)([a-zA-ZäöüÄÖÜß])', \"\\g<1>. \\g<2>\\g<3>\", content)\n",
    "    \n",
    "    content = content.replace(\",.\",\".\")\n",
    "    content = content.replace(\":.\",\":\")\n",
    "      \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausführen der clean-Methode auf den Content jedes einzelnen Cases und Zurückschreiben des gecleanten Inhaltes in den Case selbst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for case in cases:\n",
    "    cleaned_content = clean(case['content'])\n",
    "    case['content']=cleaned_content\n",
    "    i = i+1\n",
    "    \n",
    "    if(i % 10000 == 0):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Umformen von \"cases\" in einen Pandas Dataframe zur einfacheren Abspeicherung als csv-Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_cases_df = pd.DataFrame(cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Angabe eines Pfades zur Ablage der csv-Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_file_path_csv = \"../cleaned_cases.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abspeichern des Pandas Dataframes in eine csv-Datei und anschließendes Löschen des nicht mehr benötigten Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_cases_df.to_csv(export_file_path_csv, header=True, sep='\\t', encoding=\"utf-8-sig\", index=False)\n",
    "del cleaned_cases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorbereitung zum Trainieren und späteren Ausführen des PunktSentenceTokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methode: prepare_for_split\n",
    "\n",
    "IN: Der vorzubereitende Content\n",
    "\n",
    "OUT: Der vorbereitete Content\n",
    "\n",
    "In dieser Methode werden innerhalb von runden Klammern (...) Punkte durch \"_\" ersetzt, um zu verhindern, dass der PunktSentenceTokenizer innerhalb besagter Klammern Satzgrenzen erkennt. Dies ist besonders bei Klammern der Form \"( Art.14, Abs.3)\" etc. wichtig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_split(content):   \n",
    "    regEx = r\"\\((.*?)\\)\"   # Regulärer Ausdruck um \"( )\" zu erkennen\n",
    "    matches = (re.finditer(regEx, content))   #liefert alle Übereinstimmungen des regulären Ausdrucks im Text \"content\" zurück\n",
    "    for m in matches:   #Ersetzen von \".\" durch \"_\" in den gefundenen Sätzen\n",
    "        matchedText = (m.group())\n",
    "        matchedText = matchedText.replace(\".\", \"_\")\n",
    "        content = content[:m.span()[0]] + matchedText + content[m.span()[1]:]\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstellen eines Trainings-Textes für den PunktSentenceTokenizer auf Basis der gecleanten und vorbereiteten Texte der einzelnen Cases. Hier wird auf die ersten 1000 Cases und deren Content zurückgegriffen, da ein Training über mehr Cases eine Verschlechterung der Performance des PunktSentenceTokenizers zur Folge hatte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = ''\n",
    "j = 0\n",
    "for case in cases[:1000]:\n",
    "    text_train += prepare_for_split(case['content'])\n",
    "    j+=1\n",
    "    if(j % 100 == 0):\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainieren des PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden wird ein \"PunktSentenceTokenizer\" angelegt und Parametrisiert, mit Hilfe dessen im Späteren die Sätze gesplittet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PunktTrainer()   #Anlegen eines PunktTrainers\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train(text_train)   #Trainieren des PunktTrainers auf den vorbereiteten Trainings-Text\n",
    " \n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())   #Erlernte Parameter an den \"PunktSentenceTokenizer\" übergeben\n",
    "\n",
    "# Anzahl der bereits bekannten Abkürzungen\n",
    "print(len(tokenizer._params.abbrev_types))\n",
    "\n",
    "print(\"----------------------------------------\")\n",
    "\n",
    "#Definieren zusätzlicher Abkürzungen\n",
    "abbrev = [\"Abb\", \"Abk\", \"allg\", \"eigtl\", \"gegr\", \"Nrn\", \"jmd\", \"ugs\", \"urspr\", \"usw\", \"Aufl\", \"rn\", \"sog\", \"ggfs\", \"insg\", \\\n",
    "        \"Abs\", \"GVG\", \"nr\", \"Art\", \"f\", \"ff\", '\"', \"-\", \"vgl\", \"bzw\", \"bspw\", \"etc\", \"rn\", \"strspr\", \"bverwg\", \"Anm\", \\\n",
    "        \"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \\\n",
    "        \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "\n",
    "for abb in abbrev:\n",
    "    tokenizer._params.abbrev_types.add(abb.lower())   #Hinzufügen der zusätzlichen Abkürzungen zum \"PunktSentenceTokenizer\"\n",
    "    \n",
    "\n",
    "fobj = open(\"german_abbreviations.txt\")  #Hinzufügen von 4262 deutschen Abkürzungen zum \"PunktSentenceTokenizer\"\n",
    "for line in fobj:\n",
    "    line = line.lower()\n",
    "    if(line[-1] == \".\"):\n",
    "        line = line[:-1] \n",
    "    tokenizer._params.abbrev_types.add(line)\n",
    "fobj.close()\n",
    "\n",
    "print(len(tokenizer._params.abbrev_types))   # Anzahl der nun bekannten Abkürzungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitten der vorhandene Cases, parsen und abspeichern in ein Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um einen erheblichen Geschwindigkeitsgewinn zu erhalten bietet es sich an den Dependence-Parser (spacy) auf der GPU auszuführen. Dies wird mit nachfolgendem Befehl erreicht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isUSingGPU = spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kontrolle, ob die GPU tatsächlich verwendet wird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isUSingGPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laden von \"de_core_news_lg\" in spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_lg')\n",
    "nlp.max_length = 150000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methode: get_case_dict\n",
    "\n",
    "IN: Der Bereich innerhalb der Liste \"cases\", für welchen das case_dict angelegt werden soll.\n",
    "\n",
    "OUT: Das erzeugte case_dict für den vorgegebene Bereich der Liste \"cases\"\n",
    "\n",
    "In dieser Funktion wird für den jeweiligen Bereich der cases-Liste ein Dictionary angelegt, welches die einzelnen Cases nach Case-ID als Key unterscheidet und als Value ein weiteres Dictionary enthält. Das zweite Dictioanry besitzt als Key die jeweilige Satznummer und als Value ein Tuple. Dieses Tuple wiederum besteht aus dem Satz als Volltext und einem weiteren Dictionary, welches die Zuordnung der einzelnen Wörter des Satzes zu \"ROOT\", \"Subjekt\", \"AkkusativObjekt\", \"DativObjekt\", \"Rest\" übernimmt.\n",
    "\n",
    "Somit steht nach Ausführung dieser Funktion für die entsprechenden Cases ein Dictionary zur Verfügung, welches jeden Case anhand der Case-ID identifiziert, den Content des Cases in Sätze aufschlüsselt und die entsprechenden Sätze als Volltext, aber auch die Zuordnung der einzelnen Wörter zu oben beschriebenen Schlüsseln, enthält.\n",
    "\n",
    "Entscheidente Geschwindigkeitserhöhung bringt neben der bereits zuvor beschriebenen Verwendung der GPU für Spacy auch die Anwendung einer von Spacy zur Verfügung gestellten Pipeline, welche es ermöglicht, nicht nur mit einzelnen Sätzen, sondern mit Batches von Sätzen zu arbeiten. Mehr dazu in der schriftlichen Ausarbeitung. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_case_dict(case_range):\n",
    "    case_dict={}   #Dictionary welches die Cases anhand der Case-ID unterscheidet und ein Dictionary für Sätze enthält\n",
    "    print(\"started at: \", str(dt.datetime.now()))   #Ausgabe um die Verarbeitunszeit zu tracken\n",
    "    counter = 1   #counter für die Anzahl der bereits verarbeiteten Cases\n",
    "    isUsingGPU = spacy.require_gpu()   #Abfrage um nachvollziehen zu können ob die GPU verwendet wird\n",
    "    print(\"GPU? \", isUsingGPU)   #Ausgabe ob die GPU verwendet wird\n",
    "    for case in cases[case_range[0]:case_range[1]]:   #Durchlaufen aller Cases im angegeben Bereich\n",
    "        x=0   #Laufvariable für die Satznummer\n",
    "        temp = tokenizer.tokenize(prepare_for_split(case['content'])) #Vorbereiten des Contents und anschließendes Splitten mit dem PunktSentenceTokenizer \n",
    "        split_sentences = []   #Liste für die gesplitteten Sätze\n",
    "        for s in temp:   #Durchlaufen des Ergebnisses des PunktSentenceTokenizers für die Nachbearbeitung der Ergebnisse\n",
    "            split_sentences += s.split(\"\\n\")   #Splitten anhand \"\\n\"\n",
    "            \n",
    "        #Die in \"prepare_for_split\" hinzugefügten \"_\" wieder durch \".\" ersetzen und unnötige Leerzeichen an Satz-Anfang/ - Ende entfernen\n",
    "        split_sentences = [s.strip().replace(\"_\", \".\") for s in split_sentences]   \n",
    "        case_docs = list(nlp.pipe(split_sentences))   #Parsen der Sätze und ablegen der Ergebnisse in case_docs\n",
    "\n",
    "        sentence_dict={}   #Dictionary, welches als Key die Satznummer hat und als Value ein Tuple aus Volltext und Wortzuordnungen\n",
    "        for case_doc in case_docs:   #Durchlaufen aller geparsten Sätze\n",
    "            Worddict = {\"ROOT\": [], \"Subjekt\": [], \"AkkusativObjekt\": [], \"DativObjekt\": [], \"Rest\": []}\n",
    "            for token in case_doc:   #Zuordnen der Wörter zu den entsprechenden \"Wortarten\"\n",
    "                if token.dep_ == \"ROOT\":\n",
    "                    Worddict[\"ROOT\"].append(token.text) \n",
    "                elif token.dep_ == \"sb\":\n",
    "                    Worddict[\"Subjekt\"].append(token.text) \n",
    "                elif token.dep_ == \"oa\":\n",
    "                    Worddict[\"AkkusativObjekt\"].append(token.text) \n",
    "                elif token.dep_ == \"da\":\n",
    "                    Worddict[\"DativObjekt\"].append(token.text)\n",
    "                else:\n",
    "                    Worddict[\"Rest\"].append(token.text) \n",
    "            sentence_dict[x] = (case_doc, Worddict)   #Eintrag mit Satznummer als Key in sentence_dict hinzufügen\n",
    "            x = x+1\n",
    "        case_dict[case['id']] = sentence_dict   #Ablegen des sentence_dicts in das case_dict mit Case-ID als Key\n",
    "\n",
    "        if (counter%(case_range[1] - case_range[0]) == 0 and counter != 0):   #Debugausgabe\n",
    "            print(\"counter reached at time: \", str(dt.datetime.now()))\n",
    "            print(counter)\n",
    "        counter += 1 \n",
    "        \n",
    "    return case_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgrund der erheblichen Datei-Größe eines einzigen Case-Dicts für alle Cases ist es erforderlich diese auf mehrere (21) Case-Dicts für jeweils 5000 Cases aufzusplitten. Dies wird im nachfolgenden durchgeführt und und entsprechende Case-Dicts als JSON-Datei nummeriert abgelegt.\n",
    "\n",
    "Mit Hilfe des Tuples \"range_tuple\" wird der entsprechende Bereich in \"cases\" definiert, für welchen im aktuellen Arbeitsschritt ein Case-Dict angelegt werden soll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dicts = math.ceil(len(cases)/5000)   #Anzahl der nötigen case_dicts berechnen, \"math.ceil()\" rundet auf\n",
    "\n",
    "for i in range(num_dicts):\n",
    "    if(i<(num_dicts-1)):\n",
    "        range_tuple = (i*5000, (i+1)*5000)\n",
    "    else:\n",
    "        range_tuple = (i*5000, len(cases))\n",
    "    case_dict= get_case_dict(range_tuple)   #Funktion get_case_dict liefert das entsprechende case-dict zurück\n",
    "    case_dict_df = pd.DataFrame(case_dict)   #Umformen in einen Pandas Dataframe für eine erleichterte Ausgabe als JSON\n",
    "    export_file_path_json_case_dict = \"../case_dict_\" +str(i) +\"_1000.json\"\n",
    "    case_dict_df.to_json(export_file_path_json_case_dict, force_ascii=False, default_handler=str)\n",
    "    del case_dict   #Löschen der nicht mehr benötigten Variablen, um Arbteitsspeicher zu schonen\n",
    "    del case_dict_df   #Löschen der nicht mehr benötigten Variablen, um Arbteitsspeicher zu schonen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
