{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skript zum Erzeugen der benötigten Stopwords sowie der N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from utils import preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from stop_words import get_stop_words\n",
    "from nltk.util import ngrams\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Einlesen der Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Datensatz wird in ein Pandas Dataframe eingelesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases_df = pd.read_json('../2019-02-19_oldp_cases.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Pandas Dataframe wird in eine Python Liste umgeformt und anschließend gelöscht, um Arbeitsspeicher freizugeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = []\n",
    "for i in range(len(cases_df)) : \n",
    "    cases.append(cases_df.loc[i].to_dict())\n",
    "    \n",
    "del cases_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanen der eingelesenen Cases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methode: clean\n",
    "\n",
    "IN: Der zu cleanende Content\n",
    "\n",
    "OUT: Der gecleante Content\n",
    "\n",
    "In dieser Funktion werden unter anderem die HTML-Tags sowie Whitespaces entfernt. Zudem werden zwischen Aufzählungen und dem nachfolgenden Text jeweils ein Punkt sowie ein Leerzeichen eingefügt, um die Aufzählung vom Text zu separieren. Ist bereits ein Punkt nach der Aufzählung vorhanden, so wird nur ein Leerzeichen ergänzt.\n",
    "Potentielle Tippfehler, wie etwa \",.\" werden durch \".\" ersetzt, \":.\" durch \":\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(content):\n",
    "    soup = BeautifulSoup(content)\n",
    "    content = soup.get_text()\n",
    "    content = preprocessing.remove_whitespace(content)\n",
    "    \n",
    "    content = re.sub(r'(\\d)([a-zA-ZäöüÄÖÜß])', \"\\g<1>. \\g<2>\", content)\n",
    "    content = re.sub(r'(\\d)(\\.)([a-zA-ZäöüÄÖÜß])', \"\\g<1>\\g<2> \\g<3>\", content)\n",
    "    content = re.sub(r'(\\d)(\\„)([a-zA-ZäöüÄÖÜß])', \"\\g<1>. \\g<2>\\g<3>\", content)\n",
    "    \n",
    "    content = content.replace(\",.\",\".\")\n",
    "    content = content.replace(\":.\",\":\")\n",
    "      \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausführen der clean-Methode auf den Content jedes einzelnen Cases und anhängen der gecleanten Inhalte an den String \"text\".\n",
    "Dieser String wird im Anschluss für die Generierung der Stopwords und N-Grams benötigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "i = 0\n",
    "for case in cases:\n",
    "    text += clean(case['content'])\n",
    "    i = i+1\n",
    "    \n",
    "    if(i % 10000 == 0):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ersetzen der \"\\n\" durch Leerzeichen in \"text\" und anschließend Großschreibung zu Kleinschreibung abändern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace(\"\\n\",\" \")\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erzeugen der Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufsplitten von \"text\" in einzelne Wörter mit Hilfe des Word-Tokenizers von NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Löschen der Variablen \"text\" und \"content\", da diese im Weiteren nicht mehr benötigt werden. Dadurch wird eine Freigabe des Arbeitsspeichers ermöglicht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del text\n",
    "del cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zählen der Auftrittshäufigkeit der einzelnen Wörter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(words)\n",
    "n_words = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgeben der häufigsten 1000 Wörter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = c.most_common(n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach Überprüfung der Ausgabe stellt sich heraus, dass bis Index 46 ('kläger', 1000777) ausschließlich Stopwords auftreten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = most_common.index(('kläger', 1000777))\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords aus \"stop_words\" in \"predefined_stop_words\" ablegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predefined_stop_words = set(get_stop_words('german'))\n",
    "#print(predefined_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords aus \"nltk.corpus\" in \"nltk_stop_words\" ablegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stop_words = set(stopwords.words('german'))\n",
    "#print(nltk_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wörter aus \"most_common\" bis zum ermittelten Index in \"most_common_only_word\" ablegen.\n",
    "Anschließend alle Wörter des Korpuses, welche weniger als 3 Zeichen haben ebenfalls als Stopwords deklarieren und ebenfalls in \"most_common_only_word\" ablegen.\n",
    "Durch Verwendung von \".union\" ist sichergestellt, dass alle Stopwords nur einmal vorkommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_only_word = set([word[0] for word in most_common[:index]])\n",
    "\n",
    "most_common_only_word = most_common_only_word.union([word for word in words if len(word)<3])\n",
    "\n",
    "print(len(most_common_only_word))\n",
    "\n",
    "#print(most_common_only_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Möglichkeit um zusätzliche benutzerdefinierte Stopwords hinzuzufügen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zusätzliche Stopwords hinzufügen\n",
    "additional_custom_stopwords = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ',,', '[', ']', 'vgl', '...', \\\n",
    "                               'ovg', 'nrw','de',\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"bzw\", \\\n",
    "                               \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\",\"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "\n",
    "\n",
    "most_common_only_word = most_common_only_word.union(additional_custom_stopwords)\n",
    "\n",
    "print(len(most_common_only_word))\n",
    "#print(most_common_only_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle vorher definierten Variablen, welche Stopwords enthalten, in \"stop_words\" mithilfe von \".union\" ablegen und \"stop_words\" anschließend in eine Liste umformen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk_stop_words.union(predefined_stop_words.union(most_common_only_word))\n",
    "stop_words = list(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"stop_words\" in ein JSON-File ablegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../all_stop_words.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(stop_words, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aus der Wort-Liste alle Stopwords herausfiltern und in \"filtered_words\" ablegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = [word for word in words if word not in stop_words]   \n",
    "#print(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"filtered_words\" in ein JSON-File ablegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../all_filtered_words.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(filtered_words, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable \"words\" freigeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zum Einlesen der \"filtered_words\" falls Kernel stirbt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../all_filtered_words.json', encoding='utf-8') as f:\n",
    "    filtered_words = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erzeugen der N-Grams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methode: getNgrams\n",
    "\n",
    "IN: Liste der Wörter \"words\", n gibt an welches N-Gram erzeugt werden soll (3-Gram, 5-Gram etc.)\n",
    "\n",
    "OUT: Ermittelte N-Grams als Pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNgrams(words ,n):\n",
    "    \n",
    "    Ngrams = ngrams(words, n)\n",
    "    \n",
    "    return(pd.Series(Ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methode: edit_to_nice_df\n",
    "\n",
    "IN: Pandas Series \"pdseries\", welche durch Methode \"getNgrams\" zurückgegeben wurde\n",
    "\n",
    "OUT: Pandas Dataframe mit den entsprechenden N-Grams und deren Häufigkeit\n",
    "\n",
    "Hier wird die Methode \"value_counts()\" der übergebene Pandas Series aufgerufen und die so erhaltene Häufigkeit der N-Grams in der Spalte \"Counts\" eines Pandas Dataframe abgelegt, zusammen mit dem entsprechenden N-Grams in der Spalte \"N_Grams\".\n",
    "Anschließend wird der Dataframe neu indiziert und zurückgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_to_nice_df(pdseries):\n",
    "    \n",
    "    df = pd.DataFrame(pdseries.value_counts(),columns=['Counts'])\n",
    "    df['N_Grams'] = df.index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methode: export_Ngrams_JSON\n",
    "\n",
    "IN: Der Pfad in dem das JSON-File abgespeichert werden soll \"path\", der Name des JSON-Files \"filename\" sowie das N-Gram selbst \"ngram\"\n",
    "\n",
    "Hier wird zunächst der Pfad, der Dateiname sowie die Datei-Endung zusammengesetzt. Anschließend wird mithilfe der von Pandas Dataframe zur Verfügung gestellten \"to_json\" Methode das N-Gram in das entsprechende JSON-File geschrieben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_Ngrams_JSON(path, filename, ngram):\n",
    "        full_filepath = path+filename+\".json\"\n",
    "        ngram.to_json(full_filepath, force_ascii=False, default_handler=str, orient='table', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erzeugung der 3- und 5-Grams.\n",
    "\n",
    "Bei 3-Grams werden hierfür alle erzeugten 3-Grams in zwei JSON-Dateien abgelegt. Diese Aufteilung erleichtert das spätere Einlesen und Pushen an Solr.\n",
    "Bei 5-Grams werden aufgrund der erheblichen Dateigröße nur die häufigsten 75% der 5-Grams in zwei JSON-Dateien geschrieben. Die Aufteilung erleichtert auch hier das Einlesen und Pushen an Solr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path_grams = \"../\"\n",
    "for i in [3, 5]:\n",
    "    if i==3:\n",
    "        drigrams_df = edit_to_nice_df(getNgrams(filtered_words, i))\n",
    "        export_Ngrams_JSON(export_path_grams, str(i)+\"grams_df_1\", drigrams_df[:int(len(drigrams_df.index) * .5)])\n",
    "        export_Ngrams_JSON(export_path_grams, str(i)+\"grams_df_2\", drigrams_df[int(len(drigrams_df.index) * .5):])\n",
    "        del drigrams_df\n",
    "    else:\n",
    "        fivegrams_df = edit_to_nice_df(getNgrams(filtered_words, i))\n",
    "        fivegrams_df = fivegrams_df[:int(len(fivegrams_df.index) * .75)]\n",
    "        export_Ngrams_JSON(export_path_grams, str(i)+\"grams_df_1\", fivegrams_df[:int(len(fivegrams_df.index) * .5)])\n",
    "        export_Ngrams_JSON(export_path_grams, str(i)+\"grams_df_2\", fivegrams_df[int(len(fivegrams_df.index) * .5):])\n",
    "        del fivegrams_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
