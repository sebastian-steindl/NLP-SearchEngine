{
 "cells": [
  {
   "source": [
    "Wir verwenden das Modul gensim um ein Word2Vec Modell zu trainieren"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "import math\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json"
   ]
  },
  {
   "source": [
    "Über diese Funtkion können die Umgebungen aus dem Skipgram vorhergesagt werden"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import exp, dot\n",
    "from gensim import matutils\n",
    "#https://github.com/RaRe-Technologies/gensim/issues/2152\n",
    "def predict_output_context(model, center_word, topn=10):\n",
    "    word = model.wv.vocab[center_word]\n",
    "    vec = model.wv.vectors[word.index]\n",
    "    prob_values = exp(dot(vec, model.trainables.syn1neg.T))\n",
    "    prob_values /= sum(prob_values)\n",
    "    top_indices = matutils.argsort(prob_values, topn=topn, reverse=True)\n",
    "    return [(model.wv.index2word[index1], prob_values[index1]) for index1 in top_indices]"
   ]
  },
  {
   "source": [
    "Wir haben Modelle mit und ohne Stopwords erzeugt. \n",
    "\n",
    "Laden der Stopwords von nltk und der aus den Daten erzeugten Stopwords."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltkStopWords = stopwords.words(\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"all_stop_words.json\", \"r\") as f:\n",
    "    createdStopWords = json.load(f)"
   ]
  },
  {
   "source": [
    "Diese Funktion nimmt eine Liste als Parameter und generiert daraus eine RegEx die jeden String der Liste matcht "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildRegexFromList(stringsToMatch):\n",
    "    init = \"\"\n",
    "    for s in stringsToMatch:\n",
    "        init += r'\\b'\n",
    "        init += re.escape(s)\n",
    "        init += r'\\b|'\n",
    "    #remove last pipe:\n",
    "    init = init[:-1]\n",
    "    return init"
   ]
  },
  {
   "source": [
    "Regex um beispielsweise Paragraphenzeichen zu entfernen"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexNoSpecialStuff = r\"[^A-zäöüÄOÜß\\s]\""
   ]
  },
  {
   "source": [
    "Erzeugen einer Liste aller Stopwords "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltkStopWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c0b0d2d9e78a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mregexNoSpecialStuff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"[^A-zäöüß\\s]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnewStop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0madditional_custom_stopwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnewStop\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnltkStopWords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mnltkStopWords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewStop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nltkStopWords' is not defined"
     ]
    }
   ],
   "source": [
    "additional_custom_stopwords = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ',,', '[', ']', 'vgl', '...', \\\n",
    "                               'ovg', 'nrw','de',\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"bzw\", \\\n",
    "                               \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\",\"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "\n",
    "\n",
    "for newStop in additional_custom_stopwords:\n",
    "    if newStop not in nltkStopWords:\n",
    "        nltkStopWords.append(newStop)\n",
    "        \n",
    "for newStop in createdStopWords:\n",
    "    if newStop not in nltkStopWords:\n",
    "        nltkStopWords.append(newStop)"
   ]
  },
  {
   "source": [
    "Daraus die RegEx generieren"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltkStopWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c838209e6405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mregExStopWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuildRegexFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltkStopWords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nltkStopWords' is not defined"
     ]
    }
   ],
   "source": [
    "regExStopWords = buildRegexFromList(nltkStopWords)"
   ]
  },
  {
   "source": [
    "Alle Sätze laden"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../allSentences.pkl\", \"rb\") as f:\n",
    "    sentences = pickle.load(f)"
   ]
  },
  {
   "source": [
    "1/4  der Daten wird zufällig behalten, der Rest gelöscht"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retain = math.floor(len(sentences)/4)\n",
    "smallerSents = np.random.choice(sentences, retain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del smallerSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "concated = []"
   ]
  },
  {
   "source": [
    "Sätze lowern und mit Regex bereinigen, hier können Stopwords entfernt weden oder auch nicht"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanSentence(sent):\n",
    "    sent = sent.lower()\n",
    "#    sent = re.sub(regExStopWords, \"\", sent)\n",
    "    sent = re.sub(regexNoSpecialStuff, \"\", sent)\n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26160"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smallerSents)"
   ]
  },
  {
   "source": [
    "Da die Sätze in einem nested Array sind, wobei jeder Eintrag in der ersten Ebene einem Dokument entspricht, iterieren wir über die Sätze und wenden sowohl die cleanSentence Funktion als auch word_tokenize von nltk an, um alle Sätze zu cleanen und tokenizieren"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(smallerSents)):\n",
    "    concated.append([nltk.word_tokenize(cleanSentence(s)) for s in smallerSents[i]])\n",
    "    if (i%1000 ==0):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del smallerSents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26160"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(concated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Test', 'Satz']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(\"Test Satz\", language=\"german\")\n"
   ]
  },
  {
   "source": [
    "Wir nehmen nur Sätze, die mehr als 3 Worte enthalten"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n"
     ]
    }
   ],
   "source": [
    "allTokenized = []\n",
    "for idx, part in enumerate(concated):\n",
    "    for sent in part:\n",
    "        if(len(sent)>3):\n",
    "            allTokenized.append(sent)\n",
    "            \n",
    "    if (idx%100 == 0):\n",
    "        print(idx)"
   ]
  },
  {
   "source": [
    "Damit wird jetzt ein word2vec Model trainiert. Dabei werden nur Wörter die mindestens 5 mal Vorkommen berücksichtigt. Der Parameter sg=1 aktiviert SkipGram statt CBOW"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(allTokenized, min_count=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=198337, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beschwerde', 'antragsteller', 'beschluss', 'sozialgerichts', 'duisburg', 'geändert', 'antragsgegnerin', 'einstweilig', 'verpflichtet', 'antragstellern']\n"
     ]
    }
   ],
   "source": [
    "print(list(model.wv.vocab)[:10])"
   ]
  },
  {
   "source": [
    "Speichern der Modelle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"withStopwords_w2v.model\")"
   ]
  },
  {
   "source": [
    "Hier einige Test-Ausgaben"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kursiv', 0.7948729991912842),\n",
       " ('schriftgröße', 0.7640509605407715),\n",
       " ('drucktechnisch', 0.7635499238967896),\n",
       " ('sperrschrift', 0.740163266658783),\n",
       " ('überschriften', 0.7324076890945435),\n",
       " ('text', 0.7287232875823975),\n",
       " ('fließtext', 0.7165791988372803),\n",
       " ('farblich', 0.7145205736160278),\n",
       " ('kleingedruckten', 0.7116425037384033),\n",
       " ('widerrufsfolgen', 0.7100821733474731)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"fettdruck\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gefängnis', 0.020155497),\n",
       " ('gefoltert', 0.003620692),\n",
       " ('festgenommen', 0.0031493008),\n",
       " ('bestraft', 0.0030065014),\n",
       " ('geflohen', 0.0028015247),\n",
       " ('batman', 0.002466482),\n",
       " ('geflüchtet', 0.0022763843),\n",
       " ('zurückgekehrt', 0.0021883382)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_output_context(model=model, center_word=\"gefängnis\".lower(), topn=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3207576"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06819087, -0.05849149, -0.44034144,  0.43365505,  0.35931936,\n",
       "        0.23948576,  0.21274006, -0.11572772, -0.09004484,  0.23248108,\n",
       "        0.1180741 , -0.4541765 , -0.19107921, -0.22647363,  0.24266577,\n",
       "       -0.11351085,  0.13632649,  0.09431722,  0.1741414 , -0.2821583 ,\n",
       "       -0.20260008, -0.24789643, -0.06995609, -0.33414626, -0.72408265,\n",
       "       -0.17213352, -0.45253775,  0.3872577 ,  0.46451366,  0.247034  ,\n",
       "       -0.22019464, -0.5967129 ,  0.28587347,  0.26321846,  0.24335924,\n",
       "        0.52247864,  0.12950346,  0.7205421 , -0.27208588,  0.3231255 ,\n",
       "        0.23748551, -0.04112607, -0.6253382 , -0.252556  , -0.08153218,\n",
       "        0.7452898 ,  0.02241766, -0.5604143 ,  0.4858182 , -0.18336153,\n",
       "        0.49712628,  0.16229424,  0.02963648,  0.23470663, -0.6413679 ,\n",
       "        0.07037396,  0.03973873,  0.44359607, -0.43676433, -0.017108  ,\n",
       "       -0.05136756, -0.48984584,  0.10393927, -0.5248685 ,  0.5380487 ,\n",
       "       -0.4540248 , -0.25822267, -0.07911491, -0.12509339,  0.20714974,\n",
       "       -0.52661496, -0.4428308 , -0.34855878, -0.20883928,  0.43894973,\n",
       "        0.01948296,  0.22191408,  0.4638186 , -0.30449465,  0.4655562 ,\n",
       "       -0.22924584, -0.36815664,  0.14769964,  0.6365098 , -0.42631367,\n",
       "       -0.341752  ,  0.12613337,  0.12099322, -0.6422477 ,  0.23098616,\n",
       "       -0.24902703,  0.07563937, -0.02295391,  0.0411532 , -0.09347114,\n",
       "       -0.3943368 , -0.7246751 ,  0.5916081 , -0.22831097,  0.8755798 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.get_vector(\"antrag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vocabulary.sorted_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}